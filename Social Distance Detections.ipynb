{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Social Distance Detections.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"mount_file_id":"1XlQrwygFzT-LTbc1OAZsgnVkhr9Nw-xr","authorship_tag":"ABX9TyMXe7nAyhKr1sFLR06uNZ1S"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"5CGFOgQZGh0c","executionInfo":{"status":"ok","timestamp":1607601393657,"user_tz":-330,"elapsed":1181,"user":{"displayName":"Sagar Kateja","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWsM4ivrm3oPnJvKcH38_MKvlPH2pvFnAxIXwfag=s64","userId":"01326674238845029239"}}},"source":["MIN_CONF = 0.3 # to filter the detections below this\r\n","NMS_THRESH = 0.3 #For drawing the boxes over the detections\r\n","MIN_DISTANCE = 50"],"execution_count":49,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7rTs_BabJA6V"},"source":["**DETECT FUNCTION**"]},{"cell_type":"code","metadata":{"id":"2IlClkyNIemx","executionInfo":{"status":"ok","timestamp":1607601394135,"user_tz":-330,"elapsed":1642,"user":{"displayName":"Sagar Kateja","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWsM4ivrm3oPnJvKcH38_MKvlPH2pvFnAxIXwfag=s64","userId":"01326674238845029239"}}},"source":["import numpy as np\r\n","import cv2\r\n","\r\n","def detect_people(frame, net, ln, personIdx=0):\r\n","\t# grab the dimensions of the frame and  initialize the list of\r\n","\t# results\r\n","\t(H, W) = frame.shape[:2]\r\n","\tresults = []\r\n","\r\n","\t# construct a blob from the input frame and then perform a forward\r\n","\t# pass of the YOLO object detector, giving us our bounding boxes\r\n","\t# and associated probabilities\r\n","\tblob = cv2.dnn.blobFromImage(frame, 1 / 255.0, (416, 416),\r\n","\t\tswapRB=True, crop=False)\r\n","\tnet.setInput(blob)\r\n","\tlayerOutputs = net.forward(ln)\r\n","\r\n","\t# initialize our lists of detected bounding boxes, centroids, and\r\n","\t# confidences, respectively\r\n","\tboxes = []\r\n","\tcentroids = []\r\n","\tconfidences = []\r\n","\r\n","\t# loop over each of the layer outputs\r\n","\tfor output in layerOutputs:\r\n","\t\t# loop over each of the detections\r\n","\t\tfor detection in output:\r\n","\t\t\t# extract the class ID and confidence (i.e., probability)\r\n","\t\t\t# of the current object detection\r\n","\t\t\tscores = detection[5:]\r\n","\t\t\tclassID = np.argmax(scores)\r\n","\t\t\tconfidence = scores[classID]\r\n","\r\n","\t\t\t# filter detections by (1) ensuring that the object\r\n","\t\t\t# detected was a person and (2) that the minimum\r\n","\t\t\t# confidence is met\r\n","\t\t\tif classID == personIdx and confidence > MIN_CONF:\r\n","\t\t\t\t# scale the bounding box coordinates back relative to\r\n","\t\t\t\t# the size of the image, keeping in mind that YOLO\r\n","\t\t\t\t# actually returns the center (x, y)-coordinates of\r\n","\t\t\t\t# the bounding box followed by the boxes' width and\r\n","\t\t\t\t# height\r\n","\t\t\t\tbox = detection[0:4] * np.array([W, H, W, H])\r\n","\t\t\t\t(centerX, centerY, width, height) = box.astype(\"int\")\r\n","\r\n","\t\t\t\t# use the center (x, y)-coordinates to derive the top\r\n","\t\t\t\t# and and left corner of the bounding box\r\n","\t\t\t\tx = int(centerX - (width / 2))\r\n","\t\t\t\ty = int(centerY - (height / 2))\r\n","\r\n","\t\t\t\t# update our list of bounding box coordinates,\r\n","\t\t\t\t# centroids, and confidences\r\n","\t\t\t\tboxes.append([x, y, int(width), int(height)])\r\n","\t\t\t\tcentroids.append((centerX, centerY))\r\n","\t\t\t\tconfidences.append(float(confidence))\r\n","\r\n","\t# apply non-maxima suppression to suppress weak, overlapping\r\n","\t# bounding boxes\r\n","\tidxs = cv2.dnn.NMSBoxes(boxes, confidences, MIN_CONF, NMS_THRESH)\r\n","\r\n","\t# ensure at least one detection exists\r\n","\tif len(idxs) > 0:\r\n","\t\t# loop over the indexes we are keeping\r\n","\t\tfor i in idxs.flatten():\r\n","\t\t\t# extract the bounding box coordinates\r\n","\t\t\t(x, y) = (boxes[i][0], boxes[i][1])\r\n","\t\t\t(w, h) = (boxes[i][2], boxes[i][3])\r\n","\r\n","\t\t\t# update our results list to consist of the person\r\n","\t\t\t# prediction probability, bounding box coordinates,\r\n","\t\t\t# and the centroid\r\n","\t\t\tr = (confidences[i], (x, y, x + w, y + h), centroids[i])\r\n","\t\t\tresults.append(r)\r\n","\r\n","\t# return the list of results\r\n","\treturn results"],"execution_count":50,"outputs":[]},{"cell_type":"code","metadata":{"id":"zJkY5gloPVLZ","executionInfo":{"status":"ok","timestamp":1607601394139,"user_tz":-330,"elapsed":1634,"user":{"displayName":"Sagar Kateja","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWsM4ivrm3oPnJvKcH38_MKvlPH2pvFnAxIXwfag=s64","userId":"01326674238845029239"}}},"source":["#MODEL_PATH = \"yolo-coco\"\r\n","\r\n","# initialize minimum probability to filter weak detections along with\r\n","# the threshold when applying non-maxima suppression\r\n","#MIN_CONF = 0.3\r\n","#NMS_THRESH = 0.3\r\n","\r\n","# boolean indicating if NVIDIA CUDA GPU should be used\r\n","#USE_GPU = False\r\n","\r\n","# define the minimum safe distance (in pixels) that two people can be\r\n","# from each other\r\n","#MIN_DISTANCE = 50\r\n"],"execution_count":51,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M6nfob5JPWqx","executionInfo":{"status":"ok","timestamp":1607602441382,"user_tz":-330,"elapsed":1048863,"user":{"displayName":"Sagar Kateja","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjWsM4ivrm3oPnJvKcH38_MKvlPH2pvFnAxIXwfag=s64","userId":"01326674238845029239"}},"outputId":"6675f4ae-7443-4215-e4a8-699aa1ddf302"},"source":["from scipy.spatial import distance as dist\r\n","from google.colab.patches import cv2_imshow\r\n","import numpy as np\r\n","import argparse\r\n","import imutils\r\n","import cv2\r\n","import os\r\n","\r\n","# construct the argument parse and parse the arguments\r\n","ap = argparse.ArgumentParser()\r\n","ap.add_argument(\"-i\", \"--input\", type=str, default=\"\",\r\n","\thelp=\"path to (optional) input video file\")\r\n","ap.add_argument(\"-o\", \"--output\", type=str, default=\"\",\r\n","\thelp=\"path to (optional) output video file\")\r\n","ap.add_argument(\"-d\", \"--display\", type=int, default=1,\r\n","\thelp=\"whether or not output frame should be displayed\")\r\n","args = vars(ap.parse_args([\"--input\" , \"/content/drive/MyDrive/Colab Notebooks/pedestrians.mp4\",\"--output\" , \"/content/drive/MyDrive/Colab Notebooks/myOutput.avi\", \"--display\", \"1\"]))\r\n","\r\n","# load the COCO class labels our YOLO model was trained on\r\n","labelsPath = os.path.sep.join([ \"/content/drive/MyDrive/Colab Notebooks/yolo-coco/coco.names\"])\r\n","LABELS = open(labelsPath).read().strip().split(\"\\n\")\r\n","\r\n","# derive the paths to the YOLO weights and model configuration\r\n","weightsPath = os.path.sep.join([ \"/content/drive/MyDrive/Colab Notebooks/yolo-coco/yolov3.weights\"])\r\n","configPath = os.path.sep.join([\"/content/drive/MyDrive/Colab Notebooks/yolo-coco/yolov3.cfg\"])\r\n","\r\n","# load our YOLO object detector trained on COCO dataset (80 classes)\r\n","print(\"[INFO] loading YOLO from disk...\")\r\n","net = cv2.dnn.readNetFromDarknet(configPath, weightsPath)\r\n","\r\n","# check if we are going to use GPU\r\n","#if config.USE_GPU:\r\n","#\t# set CUDA as the preferable backend and target\r\n","#\tprint(\"[INFO] setting preferable backend and target to CUDA...\")\r\n","#\tnet.setPreferableBackend(cv2.dnn.DNN_BACKEND_CUDA)\r\n","#\tnet.setPreferableTarget(cv2.dnn.DNN_TARGET_CUDA)\r\n","\r\n","# determine only the *output* layer names that we need from YOLO\r\n","ln = net.getLayerNames()\r\n","ln = [ln[i[0] - 1] for i in net.getUnconnectedOutLayers()]\r\n","\r\n","# initialize the video stream and pointer to output video file\r\n","print(\"[INFO] accessing video stream...\")\r\n","vs = cv2.VideoCapture(args[\"input\"] if args[\"input\"] else 0)\r\n","writer = None\r\n","\r\n","# loop over the frames from the video stream\r\n","while True:\r\n","\t# read the next frame from the file\r\n","\t(grabbed, frame) = vs.read()\r\n","\r\n","\t# if the frame was not grabbed, then we have reached the end\r\n","\t# of the stream\r\n","\tif not grabbed:\r\n","\t\tbreak\r\n","\r\n","\t# resize the frame and then detect people (and only people) in it\r\n","\tframe = imutils.resize(frame, width=700)\r\n","\tresults = detect_people(frame, net, ln,\r\n","\t\tpersonIdx=LABELS.index(\"person\"))\r\n","\r\n","\t# initialize the set of indexes that violate the minimum social\r\n","\t# distance\r\n","\tviolate = set()\r\n","\r\n","\t# ensure there are *at least* two people detections (required in\r\n","\t# order to compute our pairwise distance maps)\r\n","\tif len(results) >= 2:\r\n","\t\t# extract all centroids from the results and compute the\r\n","\t\t# Euclidean distances between all pairs of the centroids\r\n","\t\tcentroids = np.array([r[2] for r in results])\r\n","\t\tD = dist.cdist(centroids, centroids, metric=\"euclidean\")\r\n","\r\n","\t\t# loop over the upper triangular of the distance matrix\r\n","\t\tfor i in range(0, D.shape[0]):\r\n","\t\t\tfor j in range(i + 1, D.shape[1]):\r\n","\t\t\t\t# check to see if the distance between any two\r\n","\t\t\t\t# centroid pairs is less than the configured number\r\n","\t\t\t\t# of pixels\r\n","\t\t\t\tif D[i, j] < MIN_DISTANCE:\r\n","\t\t\t\t\t# update our violation set with the indexes of\r\n","\t\t\t\t\t# the centroid pairs\r\n","\t\t\t\t\tviolate.add(i)\r\n","\t\t\t\t\tviolate.add(j)\r\n","\r\n","\t# loop over the results\r\n","\tfor (i, (prob, bbox, centroid)) in enumerate(results):\r\n","\t\t# extract the bounding box and centroid coordinates, then\r\n","\t\t# initialize the color of the annotation\r\n","\t\t(startX, startY, endX, endY) = bbox\r\n","\t\t(cX, cY) = centroid\r\n","\t\tcolor = (0, 255, 0)\r\n","\r\n","\t\t# if the index pair exists within the violation set, then\r\n","\t\t# update the color\r\n","\t\tif i in violate:\r\n","\t\t\tcolor = (0, 0, 255)\r\n","\r\n","\t\t# draw (1) a bounding box around the person and (2) the\r\n","\t\t# centroid coordinates of the person,\r\n","\t\tcv2.rectangle(frame, (startX, startY), (endX, endY), color, 2)\r\n","\t\tcv2.circle(frame, (cX, cY), 5, color, 1)\r\n","\r\n","\t# draw the total number of social distancing violations on the\r\n","\t# output frame\r\n","\ttext = \"Social Distancing Violations: {}\".format(len(violate))\r\n","\tcv2.putText(frame, text, (10, frame.shape[0] - 25),\r\n","\t\tcv2.FONT_HERSHEY_SIMPLEX, 0.85, (0, 0, 255), 3)\r\n","\r\n","\t# check to see if the output frame should be displayed to our\r\n","\t# screen\r\n","\tif args[\"display\"] > 0:\r\n","\t\t# show the output frame\r\n","#\t\tcv2.imshow(\"Frame\", frame)\r\n","\t\tkey = cv2.waitKey(1) & 0xFF\r\n","\r\n","\t\t# if the `q` key was pressed, break from the loop\r\n","\t\tif key == ord(\"q\"):\r\n","\t\t\tbreak\r\n","\r\n","\t# if an output video file path has been supplied and the video\r\n","\t# writer has not been initialized, do so now\r\n","\tif args[\"output\"] != \"\" and writer is None:\r\n","\t\t# initialize our video writer\r\n","\t\tfourcc = cv2.VideoWriter_fourcc(*\"MJPG\")\r\n","\t\twriter = cv2.VideoWriter(args[\"output\"], fourcc, 25,\r\n","\t\t\t(frame.shape[1], frame.shape[0]), True)\r\n","\r\n","\t# if the video writer is not None, write the frame to the output\r\n","\t# video file\r\n","\tif writer is not None:\r\n","\t\twriter.write(frame)"],"execution_count":52,"outputs":[{"output_type":"stream","text":["[INFO] loading YOLO from disk...\n","[INFO] accessing video stream...\n"],"name":"stdout"}]}]}